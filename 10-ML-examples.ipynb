{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ данных и алгоритмы машинного обучения\n",
    "\n",
    "1. Математические библиотеки Python и их применение для анализа/предобработки данных - пример полиномиальной регрессии\n",
    "1. Кластеризация данных и ее роль в отборе признаков и пре-процессинге\n",
    "2. Анализ продуктовой корзины\n",
    "\n",
    "__Для работы потребуются следующе датасеты:__\n",
    "- [data/web_traffic.tsv](https://github.com/easyise/spec_python_courses/raw/master/python04-analysis/data/web_traffic.tsv)\n",
    "- [data/store_data.csv](https://github.com/easyise/spec_python_courses/raw/master/python04-analysis/data/store_data.csv)\n",
    "\n",
    "Для некоторых датасетов, которые мы будем сегодня загружать из интернета, может потребоваться порядка 500МБ дискового простанства.\n",
    "\n",
    "\n",
    "__ВНИМАНИЕ__! Установите библиотеки scipy, sklearn и mlxtend: ```pip install scipy sklearn mlxtend```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy as sp\n",
    "import statsmodels.api as sm\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример применения полиномиальной регрессии для моделирования данных\n",
    "\n",
    "Допустим, у нас есть ежечасная статистика веб-траффика по некоторому серверу. Нам нужно определить, когда по времени, с учетом текущей динамики, количество запросов превысит 50000/час, чтобы заранее проапгрейдить оборудование. Для этого мы попытаемся построить кривую с помощью полиномиальной регрессии и экстраполируя ее на будущее, определим крайний срок для апгрейда оборудования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_traffic = pd.read_csv('data/web_traffic.tsv', sep='\\t', header=None, names=['Hour', 'ReqsPerHour'], index_col='Hour')\n",
    "web_traffic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберемся с пропущенными данными:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_traffic.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_traffic.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_traffic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию для красивого отображения данных и моделей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_models(x, y, models, mx=None, ymax=None, xmin=None):\n",
    "    ''' plot input data '''\n",
    "    \n",
    "    colors = ['g', 'k', 'b', 'm', 'r']\n",
    "    linestyles = ['-', '-.', '--', ':', '-']\n",
    "\n",
    "    plt.figure(num=None, figsize=(10, 6))\n",
    "    plt.clf()\n",
    "    \n",
    "    plt.scatter(x, y, s=10)\n",
    "    \n",
    "    plt.title(\"Web traffic over the last month\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Hits/hour\")\n",
    "    plt.xticks(\n",
    "        [w * 7 * 24 for w in range(10)], ['week %i' % w for w in range(10)])\n",
    "\n",
    "    if models:\n",
    "        if mx is None:\n",
    "            mx = np.linspace(0, x.shape[0], 1000)\n",
    "        for model, style, color in zip(models, linestyles, colors):\n",
    "            # print \"Model:\",model\n",
    "            # print \"Coeffs:\",model.coeffs\n",
    "            plt.plot(mx, model(mx), linestyle=style, linewidth=2, c=color)\n",
    "\n",
    "        plt.legend([\"d=%i\" % m.order for m in models], loc=\"upper left\")\n",
    "\n",
    "    plt.autoscale(tight=True)\n",
    "    plt.ylim(ymin=0)\n",
    "    if ymax:\n",
    "        plt.ylim(ymax=ymax)\n",
    "    if xmin:\n",
    "        plt.xlim(xmin=xmin)\n",
    "    plt.grid(True, linestyle='-', color='0.75')\n",
    "    \n",
    "plot_models(web_traffic.index, web_traffic.ReqsPerHour, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = web_traffic.index, web_traffic.ReqsPerHour\n",
    "\n",
    "fp1, res1, rank1, sv1, rcond1 = np.polyfit(x, y, 1, full=True)\n",
    "print(\"Model parameters of fp1: %s\" % fp1)\n",
    "print(\"Error of the model of fp1:\", res1)\n",
    "f1 = sp.poly1d(fp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_models(x, y, [f1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим еще несколько моделей с более высокой степенью многочлена:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp2, res2, rank2, sv2, rcond2 = np.polyfit(x, y, 2, full=True)\n",
    "print(\"Model parameters of fp2: %s\" % fp2)\n",
    "print(\"Error of the model of fp2:\", res2)\n",
    "f2 = sp.poly1d(fp2)\n",
    "f3 = sp.poly1d(np.polyfit(x, y, 3))\n",
    "f10 = sp.poly1d(np.polyfit(x, y, 10))\n",
    "f100 = sp.poly1d(np.polyfit(x, y, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нарисуем эти модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_models(x, y, [f1, f2, f3, f10, f100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Замечаем, что точка перегиба нашего графика находится примерно на середине второй недели. Повторим обучение наших моделей с этим смещением."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and plot a model using the knowledge about inflection point\n",
    "inflection = int(2.5 * 7 * 24)\n",
    "xa = x[:inflection]\n",
    "ya = y[:inflection]\n",
    "xb = x[inflection:]\n",
    "yb = y[inflection:]\n",
    "\n",
    "fa = sp.poly1d(np.polyfit(xa, ya, 1))\n",
    "fb = sp.poly1d(np.polyfit(xb, yb, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_models(x, y, [fa, fb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and plot a model using the knowledge about inflection point\n",
    "inflection = int(3.5 * 7 * 24)\n",
    "xa = x[:inflection]\n",
    "ya = y[:inflection]\n",
    "xb = x[inflection:]\n",
    "yb = y[inflection:]\n",
    "\n",
    "fa = sp.poly1d(np.polyfit(xa, ya, 1))\n",
    "fb = sp.poly1d(np.polyfit(xb, yb, 1))\n",
    "plot_models(x, y, [fa, fb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нарисуем существующие модели с экстраполяцией в недалекое будущее (до 6-й недели с начала наблюдений)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_models(\n",
    "    x, y, [f1, f2, f3, f10, f100],\n",
    "    mx=np.linspace(0 * 7 * 24, 6 * 7 * 24, 100),\n",
    "    ymax=10000, xmin=0 * 7 * 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А также создадим несколько моделей, обученных на данных только после второй точки перегиба:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb1 = fb\n",
    "fb2 = sp.poly1d(np.polyfit(xb, yb, 2))\n",
    "fb3 = sp.poly1d(np.polyfit(xb, yb, 3))\n",
    "fb10 = sp.poly1d(np.polyfit(xb, yb, 10))\n",
    "fb100 = sp.poly1d(np.polyfit(xb, yb, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нарисуем их:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_models(\n",
    "    x, y, [fb1, fb2, fb3, fb10, fb100],\n",
    "    mx=np.linspace(0 * 7 * 24, 6 * 7 * 24, 100),\n",
    "    ymax=10000, xmin=0 * 7 * 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Оценим точность \n",
    "\n",
    "Напишем функцию которая считает среднеквадратичную ошибку для модели и посмотрим, на сколько точны наши первоначальные модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(f, x, y):\n",
    "    return np.sum((f(x) - y) ** 2)\n",
    "\n",
    "print(\"Errors for the complete data set:\")\n",
    "for f in [f1, f2, f3, f10, f100]:\n",
    "    print(\"Error d={}: {}\" .format (f.order, error(f, x, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...и оценим их же точность, но только после точки перегиба:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Errors for only the time after inflection point\")\n",
    "for f in [f1, f2, f3, f10, f100]:\n",
    "    print(\"Error d={}: {}\" .format(f.order, error(f, xb, yb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...и теперь точность моделей, обученных после точки перегиба:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Errors for only the time after inflection point\")\n",
    "for f in [fb1, fb2, fb3, fb10, fb100]:\n",
    "    print(\"Error d=%i: %f\" % (f.order, error(f, xb, yb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выберем победительницей модель с полиномом в степени 2. Рассчитаем дату достижения предела в 50000 запросов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fsolve\n",
    "reached_max = fsolve(fb2 - 50000, x0=800) / (7 * 24)\n",
    "print(\"50,000 hits/hour expected at week %f\" % reached_max[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нарисуем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_models(\n",
    "    x, y, [fb2],\n",
    "    mx=np.linspace(0 * 7 * 24, 8 * 7 * 24, 100),\n",
    "    ymax=50000, xmin=0 * 7 * 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример 2. Кластеризация \n",
    "\n",
    "В качестве визуальной оценки данных используется кластеризация."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод PCA (метод главных компонент) позволяет уменьшить размерность датасета до 2 (или 3).  Это позволяет визуально оценить \"обучаемость\" алгоритмов на этих данных. Также метод позволяет выяснить, из чего состоят итоговые компоненты. В основе этого метода лежит сингулярное разложение векторов (SVD). Рассмотрим на примере набора данных \"Ирисы\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "print(\"Meaning of the 2 components:\")\n",
    "for component in pca.components_:\n",
    "    print(\" + \".join(\"%.3f x %s\" % (value, name)\n",
    "                     for value, name in zip(component,\n",
    "                                            iris.feature_names)))\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, s=70, cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "fig = plt.figure(figsize=(12,10))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter( X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=y, cmap=plt.cm.get_cmap('viridis'))\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим на примере датасета \"рукописные цифры\". Здесь размерность уменьшена с 64 до 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "print('Projecting %d-dimensional data to 2D' % X.shape[1])\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, \n",
    "            edgecolor='none', alpha=0.7, s=40,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "fig = plt.figure(figsize=(12,10))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter( X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=y, cmap=plt.cm.get_cmap('nipy_spectral'))\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример кластеризации по t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "tsne = TSNE()\n",
    "X_embedded = tsne.fit_transform(X)\n",
    "\n",
    "sns.scatterplot(x=X_embedded[:,0], y=X_embedded[:,1], hue=y, legend='full', palette=sns.color_palette(\"bright\", 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=3)\n",
    "X_embedded = tsne.fit_transform(X)\n",
    "\n",
    "ig = plt.figure(figsize=(12,10))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter( X_embedded[:, 0], X_embedded[:, 1], X_embedded[:, 2], c=y, cmap=plt.cm.get_cmap('nipy_spectral'))\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример 3. Анализ покупательской корзины\n",
    "\n",
    "Это пример \"майнинга данных\" - поиска зависимостей и закономерностей в массиве данных. \"Анализ покупательской корзины\" - довольно часто использующийся метод при построении рекомендательных систем. Базовый алгоритм называется ```Apriori```, он был предложен в 1994 году.\n",
    "\n",
    "Назначение алгоритма - поиск часто встречающихся подмножеств. Он оперирует следующими понятиями:\n",
    " - \"суппорт\" $Support A$ - вероятность покупки товара A, ее можно вычислить как отношение количества покупок A к общему количеству покупок.\n",
    " - \"конфидент\" $Conf  A{\\rightarrow}B$  - вычисляется для пары товаров A и B как отношение случаев совместного приобретения этих товаров к покупкам артикула A\n",
    " - \"подъем\" $Lift  A{\\rightarrow}B$  - это отношение вероятности приобретения пары товаров A и B к вероятности приобретения товара A. Или попросту говоря, отношение \"конфидента\" к \"суппорту\".\n",
    " \n",
    "Давайте \"вытащим\" из датасета с данными о покупках в некотором французском супермаркете самые популярные товары и их сочетания с помощью алгоритма ```Apriori```.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитаем датасет и посмотрим на его содержимое:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_store = pd.read_csv('data/store_data.csv', header=None)\n",
    "print(df_store.shape)\n",
    "df_store.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на список артикулов и их количество:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_store.stack().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На базе исходного dataframe создадим dataframe, в котором признаки приобретения того или иного товара станут булевыми dummy признаками. Для этого мы ему сначала сделаем ```stack()```, получим dummy-признаки, а затем сгруппируем по индексу первого уровня со взятием максимума:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies = pd.get_dummies(df_store.stack()).groupby(level=0).max()\n",
    "df_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим алгоритм Apriori и получим список наиболее часто приобретаемых товаров и их сочетаний:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "df_apriori = apriori(df_dummies, min_support=0.01, use_colnames=True)\n",
    "df_apriori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отсортируем список:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_apriori.sort_values('support', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь получим список самых популярных сочетаний для 2-х и 3-х товаров в корзине:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_apriori['item_count'] = df_apriori['itemsets'].apply(len)\n",
    "df_apriori[ df_apriori.item_count >=3 ].sort_values('support', ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Нейронные сети\n",
    "\n",
    "В рамках данной демонстрации мы, конечно, не будем углубляться во все тонкости обучения, настройки и использования нейросетей, это тема для отдельного курса. Ниже - просто пример использования сети imagenet в задаче распознавания образов: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "\n",
    "model = VGG16(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим изображение\n",
    "img_path = 'data/creative_commons_elephant.jpg'\n",
    "\n",
    "img_full = image.load_img(img_path)\n",
    "plt.imshow(img_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выполним необходимый пре-процессинг изображения и вызовем функцию predict()\n",
    "\n",
    "# `img` is a PIL image of size 224x224\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "# `x` is a float32 Numpy array of shape (224, 224, 3)\n",
    "x = image.img_to_array(img)\n",
    "\n",
    "# We add a dimension to transform our array into a \"batch\"\n",
    "# of size (1, 224, 224, 3)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "# Finally we preprocess the batch\n",
    "# (this does channel-wise color normalization)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "preds = model.predict(x)\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# узнать об архитектуре и свойствах модели можно с помощью функции summary()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
